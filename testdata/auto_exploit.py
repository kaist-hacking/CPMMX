#!/usr/bin/env python3

import logging, csv, os
import multiprocessing, subprocess
import datetime
import pandas as pd

logging.basicConfig(level=logging.INFO)

WORKER_NUM = 1
# RPC_ENDPOINT = "https://rpc.ankr.com/eth" #MAINNET
RPC_ENDPOINT = "https://rpc.ankr.com/bsc" #BSC
TIME_LIMIT = 1200

def ensure_directory_exists(directory_path):
    # Check if the directory exists
    if not os.path.exists(directory_path):
        # If it doesn't exist, create it
        os.makedirs(directory_path)
        print(f"Directory created: {directory_path}")
    else:
        print(f"Directory already exists: {directory_path}")


def parse_input_file(file_path):
    with open(file_path, 'r') as csvfile:
        input_addrs = list(csv.DictReader(csvfile))

    return input_addrs

def write_to_file(file_path, content):
    with open(file_path, 'a+') as file:
        file.write(content + ' \n')

def already_discovered(target_token_addr, base_token_addr, pair_addr):
    df = pd.read_csv('./generated_exploits.csv')
    criteria = (df['target'] == target_token_addr) & (df['pair'] == pair_addr)
    row_exists = criteria.any()
    return row_exists

def run_on_network(input_file, time_limit, result_dir_path):
    logging.info(f"file_path: {input_file}")
    logging.info(f"time_limit: {time_limit}")

    timeout_csv = os.path.join(result_dir_path, "timeout.csv")
    panic_file = os.path.join(result_dir_path, "panic_file.result")
    success_file = os.path.join(result_dir_path, "success_file.result")
    etc_csv = os.path.join(result_dir_path, "etc.csv")
    invariant_broken_but_not_profitable_csv = os.path.join(result_dir_path, "invariant_broken_but_not_profitable.csv")

    logging.info(f"timeout_file: {timeout_csv}")
    logging.info(f"panic_file: {panic_file}")
    logging.info(f"success_file: {success_file}")

    def task_function(input_addrs, time_limit):
        target_token_addr = input_addrs['target']
        base_token_addr = input_addrs['base']
        pair_addr = input_addrs['pair']
        logging.info(f"trying to generate exploit for {target_token_addr}, {base_token_addr}, {pair_addr}...")
        command = f"forge cage test {target_token_addr} {base_token_addr} {pair_addr} -f {RPC_ENDPOINT}"
        try:
            result = subprocess.run([command], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=time_limit)
            print(f"returncode: {result.returncode}")
            if result.returncode == 0:
                # Successful completion
                if not already_discovered(target_token_addr, base_token_addr, pair_addr):
                    write_to_file(success_file, f"Result for token: {target_token_addr} base: {base_token_addr} pair: {pair_addr}\n{result.stdout}")
                else:
                    print("duplicate")
            elif result.returncode == 1:
                # Err
                pass
            elif result.returncode == 134:
                # panic
                write_to_file(panic_file, f"Result for token: {target_token_addr}, base: {base_token_addr}, pair: {pair_addr}\n{result.stderr}")
            elif result.returncode == 135:
                # Could not find invariant-breaking testcase
                pass
            elif result.returncode == 136:
                write_to_file(invariant_broken_but_not_profitable_csv, f"{target_token_addr},{base_token_addr},{pair_addr}")
            else:
                # unknown return code
                write_to_file(etc_csv, f"{target_token_addr},{base_token_addr},{pair_addr}")
            
        except subprocess.TimeoutExpired or TimeoutError:
            write_to_file(timeout_csv, f"{target_token_addr},{base_token_addr},{pair_addr}")

    # Parse input file
    input_rows = parse_input_file(input_file)

    logging.info(f"input parsed, {len(input_rows)} rows")

    # Initialize Result Files
    ensure_directory_exists(result_dir_path)
    write_to_file(timeout_csv, "target,base,pair")
    write_to_file(etc_csv, "target,base,pair")
    write_to_file(invariant_broken_but_not_profitable_csv, "target,base,pair")

    processes = list()
    current_index = 0

    while current_index < len(input_rows):
        for _ in range(WORKER_NUM):
            p = multiprocessing.Process(target=task_function, args=(input_rows[current_index], time_limit))
            p.start()
            processes.append((p, input_rows[current_index]))
            current_index += 1
            if current_index >= len(input_rows):
                break

        while len(processes) > 0:
            (p, input_row) = processes.pop(0)
            p.join(timeout=time_limit)
            if p.is_alive():
                print(f"Terminating process {p.pid} due to timeout")
                p.terminate()
                p.join()
                target_token_addr = input_row['target']
                base_token_addr = input_row['base']
                pair_addr = input_row['pair']
                write_to_file(timeout_csv, f"{target_token_addr},{base_token_addr},{pair_addr}")

    print("completed all tasks")



if __name__ == "__main__":
    while True:
        today = datetime.datetime.now()
        date_str = today.strftime('%Y%m%d')

        all_pairs_path = f"{date_str}_allpairs.csv"
        tokens_path = f"{date_str}_tokens.csv"
        results_path = f"{date_str}_results"

        # Fetch Pairs
        print("Fetching pairs...")
        command = f"python3 ./crawling/crawl_bsc_pairs/0.fetch_pairs.py {all_pairs_path}"
        try:
            result = subprocess.run([command], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        except Exception as e:
            print(e)
            exit(-1)

        print(result)

        # Filter Pairs with Financial Value ($1000)
        print("Filtering pairs...")
        command = f"python3 ./crawling/crawl_bsc_pairs/crawl_pairs/1.filter_volume.py {all_pairs_path} {tokens_path}"
        try:
            result = subprocess.run([command], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        except Exception as e:
            print(e)
            exit(-1)

        run_on_network(tokens_path, TIME_LIMIT, results_path)